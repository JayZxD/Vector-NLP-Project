{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4662f163-24d1-46a1-a752-63f72d131554",
   "metadata": {},
   "source": [
    "# Clothing Review Classification Project\n",
    "## Overview\n",
    "This project focuses on analyzing and classifying clothing store reviews using Natural Language Processing (NLP) techniques. The dataset contains customer reviews with features like review text, title, rating, and recommendation flag. The goal is to preprocess the text data and build classification models to predict whether a review recommends the product or not.\n",
    "\n",
    "## Key Steps Performed:\n",
    "\n",
    "#### Data Preprocessing:\n",
    "\n",
    "- Tokenization of review text and titles\n",
    "- Stopword removal\n",
    "- Handling infrequent words\n",
    "- Vocabulary generation\n",
    "\n",
    "#### Feature Engineering:\n",
    "\n",
    "- Bag-of-Words representation\n",
    "- TF-IDF weighted vectors\n",
    "- GloVe word embeddings (both unweighted and weighted versions)\n",
    "\n",
    "#### Model Building:\n",
    "\n",
    "- Logistic Regression classifier\n",
    "- Evaluation using 5-fold cross-validation\n",
    "- Performance comparison across different feature representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428f17f7-46df-40de-965f-9b024c4eeb41",
   "metadata": {},
   "source": [
    "### Disclaimer & Usage Instructions\n",
    "\n",
    "This notebook includes a few functions that are derived from university course materials. I acknowledge their source and do not claim ownership of those specific code segments—they are used here purely for academic purposes as part of what has been taught at my university. To ensure smooth execution, please download the entire directory as it contains all the necessary files. I've included tVector.txt and vocab.txt for context; these will be automatically updated if your version of the code runs successfully.\n",
    "\n",
    "Before running this notebook, please make sure the following files are available in the directory:\n",
    "\n",
    "- Data.csv\n",
    "- stopwords_en.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e429440e-069b-4e7d-a7c1-1538992653d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19657</th>\n",
       "      <td>1104</td>\n",
       "      <td>34</td>\n",
       "      <td>Great dress for many occasions</td>\n",
       "      <td>I was very happy to snag this dress at such a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19658</th>\n",
       "      <td>862</td>\n",
       "      <td>48</td>\n",
       "      <td>Wish it was made of cotton</td>\n",
       "      <td>It reminds me of maternity clothes. soft, stre...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19659</th>\n",
       "      <td>1104</td>\n",
       "      <td>31</td>\n",
       "      <td>Cute, but see through</td>\n",
       "      <td>This fit well, but the top was very see throug...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>1084</td>\n",
       "      <td>28</td>\n",
       "      <td>Very cute dress, perfect for summer parties an...</td>\n",
       "      <td>I bought this dress for a wedding i have this ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19661</th>\n",
       "      <td>1104</td>\n",
       "      <td>52</td>\n",
       "      <td>Please make more like this one!</td>\n",
       "      <td>This dress in a lovely platinum is feminine an...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19662 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                                              Title  \\\n",
       "0             1077   60                            Some major design flaws   \n",
       "1             1049   50                                   My favorite buy!   \n",
       "2              847   47                                   Flattering shirt   \n",
       "3             1080   49                            Not for the very petite   \n",
       "4              858   39                               Cagrcoal shimmer fun   \n",
       "...            ...  ...                                                ...   \n",
       "19657         1104   34                     Great dress for many occasions   \n",
       "19658          862   48                         Wish it was made of cotton   \n",
       "19659         1104   31                              Cute, but see through   \n",
       "19660         1084   28  Very cute dress, perfect for summer parties an...   \n",
       "19661         1104   52                    Please make more like this one!   \n",
       "\n",
       "                                             Review Text  Rating  \\\n",
       "0      I had such high hopes for this dress and reall...       3   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...       5   \n",
       "2      This shirt is very flattering to all due to th...       5   \n",
       "3      I love tracy reese dresses, but this one is no...       2   \n",
       "4      I aded this in my basket at hte last mintue to...       5   \n",
       "...                                                  ...     ...   \n",
       "19657  I was very happy to snag this dress at such a ...       5   \n",
       "19658  It reminds me of maternity clothes. soft, stre...       3   \n",
       "19659  This fit well, but the top was very see throug...       3   \n",
       "19660  I bought this dress for a wedding i have this ...       3   \n",
       "19661  This dress in a lovely platinum is feminine an...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name  \\\n",
       "0                    0                        0         General   \n",
       "1                    1                        0  General Petite   \n",
       "2                    1                        6         General   \n",
       "3                    0                        4         General   \n",
       "4                    1                        1  General Petite   \n",
       "...                ...                      ...             ...   \n",
       "19657                1                        0  General Petite   \n",
       "19658                1                        0  General Petite   \n",
       "19659                0                        1  General Petite   \n",
       "19660                1                        2         General   \n",
       "19661                1                       22  General Petite   \n",
       "\n",
       "      Department Name Class Name  \n",
       "0             Dresses    Dresses  \n",
       "1             Bottoms      Pants  \n",
       "2                Tops    Blouses  \n",
       "3             Dresses    Dresses  \n",
       "4                Tops      Knits  \n",
       "...               ...        ...  \n",
       "19657         Dresses    Dresses  \n",
       "19658            Tops      Knits  \n",
       "19659         Dresses    Dresses  \n",
       "19660         Dresses    Dresses  \n",
       "19661         Dresses    Dresses  \n",
       "\n",
       "[19662 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Code to inspect the provided data file...\n",
    "data = pd.read_csv(\"Data.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ae935-d2f1-4e10-8b4f-f3b60bafd331",
   "metadata": {},
   "source": [
    "We only want to access the actual review and perform processing on that. Thus we fetch only the \"Review Text\" column from our dataframe. To understand our processing in the next steps, We decide to target only a single data entry to examine the processing as we go. Here, the index of the random data entry is set to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3781f3c9-cf88-473d-abdb-e36f812dba05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19662\n"
     ]
    }
   ],
   "source": [
    "review_data_df = data[\"Review Text\"]\n",
    "review_data = [review for review in review_data_df]\n",
    "print(len(review_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12ad68c5-417c-4cee-861f-2449bfe8d188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A flattering, super cozy coat.  will work well for cold, dry days and will look good with jeans or a dressier outfit.  i am 5' 5'', about 135 and the small fits great.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Marking a review as an example for observing the processing\n",
    "exp = 15\n",
    "review_data[exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18a06949-9fb8-4ed5-9330-f4beb36c44f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeReview(review):\n",
    "    # This function tokenizes with the given regular expression.  \n",
    "    # Cover all words to lowercase\n",
    "    new_rev = review.lower() \n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = nltk.RegexpTokenizer(pattern) \n",
    "    tokenised_review = tokenizer.tokenize(new_rev)\n",
    "    return tokenised_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221ab396-88d3-406b-8c51-561d4eff0fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'flattering',\n",
       " 'super',\n",
       " 'cozy',\n",
       " 'coat',\n",
       " 'will',\n",
       " 'work',\n",
       " 'well',\n",
       " 'for',\n",
       " 'cold',\n",
       " 'dry',\n",
       " 'days',\n",
       " 'and',\n",
       " 'will',\n",
       " 'look',\n",
       " 'good',\n",
       " 'with',\n",
       " 'jeans',\n",
       " 'or',\n",
       " 'a',\n",
       " 'dressier',\n",
       " 'outfit',\n",
       " 'i',\n",
       " 'am',\n",
       " 'about',\n",
       " 'and',\n",
       " 'the',\n",
       " 'small',\n",
       " 'fits',\n",
       " 'great']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_reviews = [tokenizeReview(review) for review in review_data]\n",
    "tokenized_reviews[exp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0911baa-9a79-4b34-889d-87fb640de41a",
   "metadata": {},
   "source": [
    "Tokenization - It is an important process where in we process the data into a simpler format for efficient modelling. It normally refers to longer strings broken into shorter strings - individual strings. We create a function - tokenizeReview with all the processing required. \n",
    "\n",
    "- Converting to lower case\n",
    "- Using RegexpTokenizer method to pattern match to remove numbers\n",
    "- Using the ntlk tokenize() method to do the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97decb4f-e0a5-475d-95ee-58e574c6a8d3",
   "metadata": {},
   "source": [
    "def tokenizeReview(review):\n",
    "    # This function tokenizes with the given regular expression.  \n",
    "    # Cover all words to lowercase\n",
    "    new_rev = review.lower() \n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = nltk.RegexpTokenizer(pattern) \n",
    "    tokenised_review = tokenizer.tokenize(new_rev)\n",
    "    return tokenised_review\n",
    "\n",
    "tokenized_reviews = [tokenizeReview(review) for review in review_data]\n",
    "tokenized_reviews[exp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c205de-0809-480e-a893-2d3306c02482",
   "metadata": {},
   "source": [
    "We also define a statistics function to keep track of statistics as we process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10e99cd-a56e-4cd5-b9b6-3e34e0a0b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a statistics function to keep track of statistics\n",
    "def stats_print(reviews):\n",
    "    # We put all the tokens in the corpus in a single list\n",
    "    words = list(chain.from_iterable(reviews)) \n",
    "    # Compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    vocab = set(words) \n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of reviews:\", len(reviews))\n",
    "    lens = [len(review) for review in reviews]\n",
    "    print(\"Average review length:\", np.mean(lens))\n",
    "    print(\"Maximun review length:\", np.max(lens))\n",
    "    print(\"Minimun review length:\", np.min(lens))\n",
    "    print(\"Standard deviation of review length:\", np.std(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f14d037-6edd-4f87-8a09-318b32487ffe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  14806\n",
      "Total number of tokens:  1206688\n",
      "Lexical diversity:  0.012269948818584423\n",
      "Total number of reviews: 19662\n",
      "Average review length: 61.37157969687723\n",
      "Maximun review length: 113\n",
      "Minimun review length: 2\n",
      "Standard deviation of review length: 27.802596969841698\n"
     ]
    }
   ],
   "source": [
    "stats_print(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379d6f3-ea2c-445f-ab6c-bdfb74188c07",
   "metadata": {},
   "source": [
    "Finally, we move to the final processing data. We'll talk about it step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4429800c-f01f-442f-93cd-ac4e7db4a2e7",
   "metadata": {},
   "source": [
    "First let's remove the words who have length less than 2. They often include words like \"is\", 'it', 'as', 'am', 'I' which often can be removed whilst maintaing the context with its previous & following words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "706b0248-6277-4e03-af24-320deb82f783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flattering',\n",
       " 'super',\n",
       " 'cozy',\n",
       " 'coat',\n",
       " 'will',\n",
       " 'work',\n",
       " 'well',\n",
       " 'for',\n",
       " 'cold',\n",
       " 'dry',\n",
       " 'days',\n",
       " 'and',\n",
       " 'will',\n",
       " 'look',\n",
       " 'good',\n",
       " 'with',\n",
       " 'jeans',\n",
       " 'or',\n",
       " 'dressier',\n",
       " 'outfit',\n",
       " 'am',\n",
       " 'about',\n",
       " 'and',\n",
       " 'the',\n",
       " 'small',\n",
       " 'fits',\n",
       " 'great']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate over tokenized_reviews and update each review\n",
    "for i, review in enumerate(tokenized_reviews):\n",
    "    # Create a new list with words of length greater than 2\n",
    "    new_rev = [word for word in review if len(word) >= 2] \n",
    "    # Update the tokenized_reviews list with the filtered words\n",
    "    tokenized_reviews[i] = new_rev\n",
    "    \n",
    "tokenized_reviews[exp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41513aff-a5d0-4cb4-90b6-7334288e0228",
   "metadata": {},
   "source": [
    "Stopwords are common words like \"the,\" \"is,\" and \"and\" that carry little meaningful information. We remove them to reduce noise and focus on the more important words that contribute to the overall meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ab46261-5492-493f-89cd-b8b06dc89292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_reviews = []\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords_reviews = f.read().splitlines()\n",
    "\n",
    "# Converting the stopwords list into a set for better time complexity\n",
    "stopwordSet = set(stopwords_reviews)\n",
    "len(stopwordSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55a070db-0245-44e7-be3a-1ed015446bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stop words\n",
    "tokenized_reviews = [[w for w in review if w not in stopwordSet] \\\n",
    "                      for review in tokenized_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c141aa-7466-43e0-bee8-9ae38cdf45d9",
   "metadata": {},
   "source": [
    "Honestly, thats pretty decent preprocessing but we process further to build upon our understanding. \n",
    "Thus, we first calculate the term frequency and the doc frequency. \n",
    "\n",
    "Term Frequency - Term Frequency (TF) measures how often a word appears in a document, helping identify important words within that text.\n",
    "\n",
    "Document Frequency - Document Frequency (DF) counts how many documents contain a word, helping assess how common or unique a word is across a collection of documents.\n",
    "\n",
    "This will smoothen the overall preprocessing process. We can do this by using the pre-existing FreqDist function in the nltk module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7284403-2874-4b1f-977d-a956d644b30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import *\n",
    "# Chaining the reviews into a single list\n",
    "review_words = list(chain.from_iterable(tokenized_reviews))\n",
    "term_fd = FreqDist(review_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4d1ed9-d2a8-402b-b7d1-c1dfa47be48d",
   "metadata": {},
   "source": [
    "Now, we remove the word that appears only once in the document collection based on the term frequency. We first fetch all the words that occur only once using the hapaxes() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "254d9538-f264-42e6-b388-f4b3ab349270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6734"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lessFreqWords = set(term_fd.hapaxes())\n",
    "len(lessFreqWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c5dfc-63a9-43bc-8038-7379f3daadac",
   "metadata": {},
   "source": [
    "We define a function to remove these words for each review. Then passing the function inside a list comprehension get the new tokenized reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6296a079-773b-4e2e-b7ab-370ee774a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLessFreqWords(review):\n",
    "    return [w for w in review if w not in lessFreqWords]\n",
    "\n",
    "tokenized_reviews = [removeLessFreqWords(review) for review in tokenized_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249ac4f-717a-4ba5-961f-223aeb31e063",
   "metadata": {},
   "source": [
    "Now calculating the document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1b02a22-12ff-445f-bafe-63acf3597f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 6416),\n",
       " ('size', 5888),\n",
       " ('fit', 5537),\n",
       " ('dress', 5346),\n",
       " ('wear', 4900),\n",
       " ('top', 4670),\n",
       " ('great', 4497),\n",
       " ('fabric', 3712),\n",
       " ('color', 3604),\n",
       " ('small', 3265),\n",
       " ('ordered', 3099),\n",
       " ('perfect', 2973),\n",
       " ('flattering', 2939),\n",
       " ('soft', 2805),\n",
       " ('comfortable', 2597),\n",
       " ('back', 2538),\n",
       " ('cute', 2398),\n",
       " ('fits', 2394),\n",
       " ('nice', 2393),\n",
       " ('bought', 2376)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chaining a new review string with unique characters in each review\n",
    "review_words_2 = list(chain.from_iterable([set(review) for review in tokenized_reviews]))\n",
    "doc_fd = FreqDist(review_words_2)  \n",
    "doc_fd.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395613d-8727-4524-a898-0d7b7bb2cee3",
   "metadata": {},
   "source": [
    "Now we do the same to remove these top 20 most common words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c0c2d3c-7912-448d-b5ae-47041187e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_docfreq = doc_fd.most_common(20)\n",
    "top20_docfreq_words = []\n",
    "# Appending only the word from the freq_list\n",
    "for word in top20_docfreq:\n",
    "    top20_docfreq_words.append(word[0])\n",
    "\n",
    "# Parsing the list for a better lookup time\n",
    "top20_docfreq_words = (top20_docfreq_words)\n",
    "\n",
    "def removeTopdocFreqWords(review):\n",
    "    return [w for w in review if w not in top20_docfreq_words]\n",
    "\n",
    "tokenized_reviews = [removeTopdocFreqWords(review) for review in tokenized_reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322c5c3-0ce9-4447-ab2d-6bab0bd5e85d",
   "metadata": {},
   "source": [
    "Taking a look at the updated statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e89eda7-e587-4817-83e6-3fe61152cf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  7529\n",
      "Total number of tokens:  355505\n",
      "Lexical diversity:  0.021178323792914306\n",
      "Total number of reviews: 19662\n",
      "Average review length: 18.080815786796865\n",
      "Maximun review length: 47\n",
      "Minimun review length: 0\n",
      "Standard deviation of review length: 8.833524535391433\n"
     ]
    }
   ],
   "source": [
    "stats_print(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eba061-b8ca-464a-9a3b-c792d26c019a",
   "metadata": {},
   "source": [
    "The vocabulary size is reduced to half indicating maximum population of the words belonged to the removed words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71a3e5e-c5f7-45cc-81fa-e4588479dbca",
   "metadata": {},
   "source": [
    "Finally, I feel we the data is processed enough. Let's do some final checks. Looking for empty reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5a4ffd3-be00-483f-b5f7-d55e635e5259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found at: index  4160\n",
      "Found at: index  6521\n",
      "Found at: index  11211\n",
      "Found at: index  11568\n",
      "Found at: index  12662\n",
      "Found at: index  12777\n",
      "Found at: index  13095\n",
      "Found at: index  14727\n",
      "Found at: index  15314\n",
      "Found at: index  18113\n"
     ]
    }
   ],
   "source": [
    "for i, review in enumerate(tokenized_reviews):\n",
    "    if review == []:\n",
    "        print(\"Found at: index \",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "520a1647-272b-4238-adef-02d431b1c169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[\"Processed Reviews\"] = tokenized_reviews\n",
    "# Filter out rows where \"Processed Reviews\" contains empty lists\n",
    "data = data[data['Processed Reviews'].apply(lambda x: len(x) > 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "060700ce-da01-4b73-8325-4d2bf93a3e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Processed Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[high, hopes, wanted, work, initially, petite,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>[jumpsuit, fun, flirty, fabulous, time, compli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>[shirt, due, adjustable, front, tie, length, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[tracy, reese, dresses, petite, feet, tall, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[basket, hte, person, store, pick, teh, pale, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19657</th>\n",
       "      <td>1104</td>\n",
       "      <td>34</td>\n",
       "      <td>Great dress for many occasions</td>\n",
       "      <td>I was very happy to snag this dress at such a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[happy, snag, price, easy, slip, cut, combo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19658</th>\n",
       "      <td>862</td>\n",
       "      <td>48</td>\n",
       "      <td>Wish it was made of cotton</td>\n",
       "      <td>It reminds me of maternity clothes. soft, stre...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[reminds, maternity, clothes, stretchy, shiny,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19659</th>\n",
       "      <td>1104</td>\n",
       "      <td>31</td>\n",
       "      <td>Cute, but see through</td>\n",
       "      <td>This fit well, but the top was very see throug...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[worked, glad, store, order, online]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>1084</td>\n",
       "      <td>28</td>\n",
       "      <td>Very cute dress, perfect for summer parties an...</td>\n",
       "      <td>I bought this dress for a wedding i have this ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[wedding, summer, medium, waist, perfectly, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19661</th>\n",
       "      <td>1104</td>\n",
       "      <td>52</td>\n",
       "      <td>Please make more like this one!</td>\n",
       "      <td>This dress in a lovely platinum is feminine an...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[lovely, feminine, perfectly, easy, comfy, hig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19652 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                                              Title  \\\n",
       "0             1077   60                            Some major design flaws   \n",
       "1             1049   50                                   My favorite buy!   \n",
       "2              847   47                                   Flattering shirt   \n",
       "3             1080   49                            Not for the very petite   \n",
       "4              858   39                               Cagrcoal shimmer fun   \n",
       "...            ...  ...                                                ...   \n",
       "19657         1104   34                     Great dress for many occasions   \n",
       "19658          862   48                         Wish it was made of cotton   \n",
       "19659         1104   31                              Cute, but see through   \n",
       "19660         1084   28  Very cute dress, perfect for summer parties an...   \n",
       "19661         1104   52                    Please make more like this one!   \n",
       "\n",
       "                                             Review Text  Rating  \\\n",
       "0      I had such high hopes for this dress and reall...       3   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...       5   \n",
       "2      This shirt is very flattering to all due to th...       5   \n",
       "3      I love tracy reese dresses, but this one is no...       2   \n",
       "4      I aded this in my basket at hte last mintue to...       5   \n",
       "...                                                  ...     ...   \n",
       "19657  I was very happy to snag this dress at such a ...       5   \n",
       "19658  It reminds me of maternity clothes. soft, stre...       3   \n",
       "19659  This fit well, but the top was very see throug...       3   \n",
       "19660  I bought this dress for a wedding i have this ...       3   \n",
       "19661  This dress in a lovely platinum is feminine an...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name  \\\n",
       "0                    0                        0         General   \n",
       "1                    1                        0  General Petite   \n",
       "2                    1                        6         General   \n",
       "3                    0                        4         General   \n",
       "4                    1                        1  General Petite   \n",
       "...                ...                      ...             ...   \n",
       "19657                1                        0  General Petite   \n",
       "19658                1                        0  General Petite   \n",
       "19659                0                        1  General Petite   \n",
       "19660                1                        2         General   \n",
       "19661                1                       22  General Petite   \n",
       "\n",
       "      Department Name Class Name  \\\n",
       "0             Dresses    Dresses   \n",
       "1             Bottoms      Pants   \n",
       "2                Tops    Blouses   \n",
       "3             Dresses    Dresses   \n",
       "4                Tops      Knits   \n",
       "...               ...        ...   \n",
       "19657         Dresses    Dresses   \n",
       "19658            Tops      Knits   \n",
       "19659         Dresses    Dresses   \n",
       "19660         Dresses    Dresses   \n",
       "19661         Dresses    Dresses   \n",
       "\n",
       "                                       Processed Reviews  \n",
       "0      [high, hopes, wanted, work, initially, petite,...  \n",
       "1      [jumpsuit, fun, flirty, fabulous, time, compli...  \n",
       "2      [shirt, due, adjustable, front, tie, length, l...  \n",
       "3      [tracy, reese, dresses, petite, feet, tall, br...  \n",
       "4      [basket, hte, person, store, pick, teh, pale, ...  \n",
       "...                                                  ...  \n",
       "19657       [happy, snag, price, easy, slip, cut, combo]  \n",
       "19658  [reminds, maternity, clothes, stretchy, shiny,...  \n",
       "19659               [worked, glad, store, order, online]  \n",
       "19660  [wedding, summer, medium, waist, perfectly, lo...  \n",
       "19661  [lovely, feminine, perfectly, easy, comfy, hig...  \n",
       "\n",
       "[19652 rows x 11 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b0708-4718-46d3-a89c-2356a34398de",
   "metadata": {},
   "source": [
    "Also, generating a vocabulary\n",
    "\n",
    "Vocabulary - In modeling, Vocabulary stands for all the unique words in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c5097d6a-ff5c-44d7-ac58-680714ec5e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable(tokenized_reviews)) \n",
    "vocab = set(words)\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for index, word in enumerate(vocab):\n",
    "        f.write(f\"{word}:{index}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d10606-10fc-48b8-a1ff-245a250fc543",
   "metadata": {},
   "source": [
    "Now that the data is cleaned, Lets move with the processing for the next steps. We'll build vector representations of the models. I have used a prebuilt glove_model with 50 dimensions for the sake of faster processing. It is a solid model but you can use any other model in that same format. Please examine the model from the working directory before using your own model\n",
    "\n",
    "Let's extract the review data into a list for vectorization. Plus combining all the review text into a continous string for bag-of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acc2b638-0410-46a8-a0c6-157695dd38dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19652\n"
     ]
    }
   ],
   "source": [
    "review_df = data['Processed Reviews']\n",
    "reviews = [' '.join(review) for review in review_df]\n",
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f294c-a878-44c9-a54f-b59a3f629524",
   "metadata": {},
   "source": [
    "Generating Feature Representation - We create a CountVector representation using the countVectorizer from the scikit learn module. We pass our vocab as the vocab for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60862fea-9cf8-4c34-833d-4e1f1a195787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19652, 7529)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cVectorizer = CountVectorizer(analyzer = \"word\",vocabulary = vocab)\n",
    "count_features = cVectorizer.fit_transform(reviews) \n",
    "count_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22f0c3-2d30-403d-9502-592675856fa6",
   "metadata": {},
   "source": [
    "Now, Similarly we can make the TF-IDF weighted vector representation by using the TF-IDF vectorizer from the sci-kit learn module.\n",
    "We'll need this later to calculate the TF-IDF word embeddings based on the Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da2006b2-24d2-4606-a80f-0e745a064696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19652, 7529)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tVectorizer = TfidfVectorizer(analyzer = \"word\",vocabulary = vocab) \n",
    "tfidf_features = tVectorizer.fit_transform(reviews) \n",
    "tfidf_features.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c1fd1800-c67b-449e-927f-22ea37aa8089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a-cup', 'a-flutter', 'a-frame', ..., 'zips', 'zone', 'zoom'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_feature_names = tVectorizer.get_feature_names_out()\n",
    "tfidf_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6558d4-65a8-46a1-9751-1fef44b082bb",
   "metadata": {},
   "source": [
    "Before we proceed, we define a function docvecs which will be used to vectorize each document from the data\n",
    "In our case, each review would be vectorized based on the embeddings passed. Finally it takes the sum of all the values (words) and appends it to the final df creating a list of vectors one for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c9e6a79-253c-47c7-a5ab-b7f5dd9208e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docvecs(embeddings, docs):\n",
    "    vecs = np.zeros((len(docs), embeddings.vector_size))\n",
    "    for i, doc in enumerate(docs):\n",
    "        valid_keys = [term for term in doc if term in embeddings.key_to_index]\n",
    "        docvec = np.vstack([embeddings[term] for term in valid_keys])\n",
    "        docvec = np.sum(docvec, axis=0)\n",
    "        vecs[i,:] = docvec\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0ebcd-0df7-4ff9-b772-7e8012bd4e29",
   "metadata": {},
   "source": [
    "Now, I have used the Glove model which I got from my University. You can use many such models which are available online - Word2Vec, FastText, etc. Make sure you inspect the Glove model from the directory before using your own model. They all should be in the same format (most cases). \n",
    "\n",
    "For the model, we first extract its embeddings into KeyedVectors. KeyedVectors - basically extract the values of its key & weights to apply to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "093a7ccb-2d60-4f77-af43-e7cea9b694d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\jay\\anaconda3\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\jay\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\jay\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\jay\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "551686d7-0ebb-441e-bf0a-6a4509ec22d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy==1.12 in c:\\users\\jay\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\jay\\anaconda3\\lib\\site-packages (from scipy==1.12) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy==1.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b8757-2131-482f-809a-ec3d9fe4c637",
   "metadata": {},
   "source": [
    "Loading the Glove model from my Hugging Face repository. Since the model exceeds 100mb - I used Hugging face to host the model\n",
    "You can use your own model here and modify this code accordingly to load your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b69e4b1-ffce-4ec3-9ad4-93293919f956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GloVe embeddings from Hugging Face...\n",
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Define the URL and local filename\n",
    "glove_url = \"https://huggingface.co/datasets/Jay-Mayekar/glove-vectors/resolve/main/glove.6B.50d.txt\"\n",
    "glove_path = \"glove.6B.50d.txt\"\n",
    "\n",
    "# Download the file if not present\n",
    "if not os.path.exists(glove_path):\n",
    "    print(\"Downloading GloVe embeddings from Hugging Face...\")\n",
    "    response = requests.get(glove_url)\n",
    "    with open(glove_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"GloVe model already exists locally.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "782cc4ed-afc2-4e55-9dbf-09d143ae328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "with open(glove_path,encoding=\"utf8\") as f:\n",
    "    keys, weights = [], []\n",
    "    for l in f:\n",
    "        k, v = l.split(maxsplit=1)\n",
    "        keys.append(k)\n",
    "        weights.append(np.fromstring(v, sep=' '))\n",
    "glove_embeddings = KeyedVectors(50, count=400000)\n",
    "glove_embeddings.add_vectors(keys, weights)\n",
    "preTrained_Glove_wv = glove_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc9b1d-16fb-4333-b108-5a9acf5cf14f",
   "metadata": {},
   "source": [
    "Now, Applying these extracted embeddings to our reviews. This gives us unweighted embeddings representation for our data. Keep in mind the \"weights\" referred previously for this glove model are the generic weights preexisting in the glove model, thus we call the output as unweighted as it does not take the \"weights\" (generated by us) into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff3b1ab5-b4c4-426a-960a-bad946c5c60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.57157302e+00, -3.96359205e+00,  3.31884074e+00, -6.83513212e+00,\n",
       "        8.01658821e+00,  5.30020332e+00, -4.29943085e+00, -1.00662270e+01,\n",
       "        2.33869982e+00,  1.21137202e+00,  2.73536777e+00, -3.02923322e-02,\n",
       "       -4.01857710e+00,  4.96555233e+00,  6.82524681e-01,  3.60882449e+00,\n",
       "        9.97775972e-01, -1.20634401e+00,  6.42051250e-02, -1.47975597e+01,\n",
       "       -1.44129407e+00, -7.41323137e+00,  5.38263083e+00, -4.94232595e-01,\n",
       "       -3.16907072e+00, -2.30365410e+01, -8.68683815e+00,  1.71912041e+01,\n",
       "        1.31793699e+01, -8.01629734e+00,  6.85179901e+01, -2.17898786e-02,\n",
       "        7.81168747e+00, -1.46274626e+00,  2.36876106e+00,  7.57402039e+00,\n",
       "       -3.80273843e+00,  1.46780090e+01,  3.06831312e+00, -9.08834839e+00,\n",
       "        2.60966492e+00,  6.33448958e-01,  6.21696711e+00,  2.84020877e+00,\n",
       "       -7.33742666e+00, -7.12737679e-01,  8.26390553e+00, -2.39080340e-01,\n",
       "        2.97417307e+00, -2.44539618e+00])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preTGloVe_dvs = docvecs(preTrained_Glove_wv , data[\"Processed Reviews\"])\n",
    "preTGloVe_dvs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1e943c76-0376-4aea-b5fa-2561b847ce76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19652, 50)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preTGloVe_dvs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff36a88-dc4c-4c7d-9b1b-699552e8b900",
   "metadata": {},
   "source": [
    "Now, lets focus on the weighted embeddings. Weights are the values that denote the significance or importance of a certain object(words in our case) in the context of the data. We calculated the weights previously by using the TF-IDF method. Let's bring them back in the picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e236a59f-cd84-40cb-9ced-d55583496b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19652"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_scores = tfidf_features.toarray()\n",
    "len(tfidf_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dabc204-5e38-4cf3-92b8-48d623e336c9",
   "metadata": {},
   "source": [
    "Generating a word_index: word dictionary from our vocab.txt We need this to generate the TF-IDF weighted vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e00c51c8-01ce-45c4-883b-0ff2fb0dc849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vocIndex(voc_fname):\n",
    "    with open(voc_fname) as vocf: \n",
    "        # Each line is 'word:index'\n",
    "        voc_Ind = [l.split(':') for l in vocf.read().splitlines()] \n",
    "        return {int(vi[1]):vi[0] for vi in voc_Ind}\n",
    "\n",
    "# Generates the w_index:word dictionary\n",
    "voc_fname = 'vocab.txt' \n",
    "voc_dict = gen_vocIndex(voc_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c55248-74cb-4391-83b7-7e90b55fabff",
   "metadata": {},
   "source": [
    "Before, we proceed with applying the glove embeddings to our TF-IDF values. Let's first save our TF-IDF vector representation into a text file which we can access later using the below function\n",
    "This function takes in the features_names and the filename for any vector and stores in word_index:value format\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "676ca8df-8552-4e0a-b911-44809b14eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vectorFile(data_features,filename):\n",
    "    # The number of articles\n",
    "    num = data_features.shape[0] \n",
    "    out_file = open(filename, 'w') \n",
    "    for a_ind in range(0, num): \n",
    "        # For each word index that has non-zero entry in the data_feature\n",
    "        for f_ind in data_features[a_ind].nonzero()[1]: \n",
    "            value = data_features[a_ind][0,f_ind] \n",
    "            # Write the entry to the file in the format of word_index:value\n",
    "            out_file.write(\"{}:{} \".format(f_ind,value)) \n",
    "        out_file.write('\\n') \n",
    "    out_file.close() \n",
    "\n",
    "write_vectorFile(tfidf_features,\"tVector.txt\")\n",
    "\n",
    "def doc_wordweights(fName_tVectors, voc_dict):\n",
    "    # A list to store the  word:weight dictionaries of documents\n",
    "    tfidf_weights = [] \n",
    "    \n",
    "    with open(fName_tVectors) as tVecf: \n",
    "        tVectors = tVecf.read().splitlines() # each line is a tfidf vector representation of a document in string format 'word_index:weight word_index:weight .......'\n",
    "        # for each tfidf document vector\n",
    "    for tv in tVectors: \n",
    "        tv = tv.strip()\n",
    "        weights = tv.split(' ') # list of 'word_index:weight' entries\n",
    "        weights = [w.split(':') for w in weights] # change the format of weight to a list of '[word_index,weight]' entries\n",
    "        wordweight_dict = {voc_dict[int(w[0])]:w[1] for w in weights} # construct the weight dictionary, where each entry is 'word:weight'\n",
    "        tfidf_weights.append(wordweight_dict) \n",
    "    return tfidf_weights\n",
    "\n",
    "fName_tVectors = 'tVector.txt'\n",
    "tfidf_weights = doc_wordweights(fName_tVectors, voc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec9802-3453-4770-8c8a-a3bccd96124a",
   "metadata": {},
   "source": [
    "Alright, That was a bit too much of code. Lets examine the tfidf_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57584986-f7c6-43b9-ba66-ff17881622a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zipper': '0.13796655643162234',\n",
       " 'zip': '0.15790936302533073',\n",
       " 'work': '0.08942754431903749',\n",
       " 'wanted': '0.10688230992054923',\n",
       " 'usual': '0.1108979967093157',\n",
       " 'tight': '0.09944270371682062',\n",
       " 'sewn': '0.15731076193390778',\n",
       " 'reordered': '0.20116611478729465',\n",
       " 'petite': '0.17993461419688872',\n",
       " 'outrageously': '0.2567966207221875',\n",
       " 'nicely': '0.11400095142158763',\n",
       " 'net': '0.4867876227942154',\n",
       " 'medium': '0.09045659947118836',\n",
       " 'major': '0.19468911285051188',\n",
       " 'layers': '0.16401760637664864',\n",
       " 'layer': '0.2738967869996706',\n",
       " 'initially': '0.17097793161240013',\n",
       " 'imo': '0.19309847172209627',\n",
       " 'hopes': '0.16844705121993458',\n",
       " 'high': '0.10509713403953778',\n",
       " 'half': '0.29992142257858656',\n",
       " 'found': '0.11016820762903567',\n",
       " 'flaw': '0.1869368578273477',\n",
       " 'fact': '0.14687037381367216',\n",
       " 'directly': '0.22042366265962396',\n",
       " 'design': '0.0981905331970358',\n",
       " 'cheap': '0.13834681733551207',\n",
       " 'bottom': '0.10490905048781156'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c4730-4e4a-4e25-8e3d-d053d4807da2",
   "metadata": {},
   "source": [
    "Finally, we define a function which will generate the weighted document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "26e3e83d-26f2-4aaa-b82d-735ff7f5cd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.59977520e-01, -1.92892775e-01, -6.64334118e-01, -5.46065681e-02,\n",
       "        1.63867283e+00, -9.06967938e-01, -6.00465298e-01,  4.24000435e-02,\n",
       "       -7.81980574e-01, -6.76829159e-01, -9.17282820e-01, -4.17724371e-01,\n",
       "       -5.99205375e-01,  9.44996119e-01,  3.76190066e-01,  8.59460473e-01,\n",
       "       -9.81933594e-01, -4.46239054e-01,  8.59710723e-02, -2.84964967e+00,\n",
       "        3.30249786e-01,  1.06721950e+00, -1.45591632e-01, -2.57114649e-01,\n",
       "       -4.13771838e-01, -2.41196752e+00, -7.88188457e-01,  2.39681339e+00,\n",
       "        2.35659385e+00, -9.96977031e-01,  7.15750122e+00,  4.26397234e-01,\n",
       "        9.89538312e-01,  1.10015762e+00, -4.04059350e-01, -1.34081542e-01,\n",
       "        2.92902291e-01,  6.53782189e-01, -2.62964636e-01, -1.15668774e+00,\n",
       "       -1.49173126e-01, -6.43850490e-03,  3.96176904e-01,  1.31000030e+00,\n",
       "        2.73277253e-01, -2.21045643e-01, -2.95024753e-01, -2.65133810e+00,\n",
       "       -1.48373589e-01,  3.03464234e-01])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weighted_docvecs(embeddings, tfidf, docs):\n",
    "    vecs = np.zeros((len(docs), embeddings.vector_size))\n",
    "    for i, doc in enumerate(docs):\n",
    "        valid_keys = [term for term in doc if term in embeddings.key_to_index]\n",
    "        tf_weights = [float(tfidf[i].get(term, 0.)) for term in valid_keys]\n",
    "        assert len(valid_keys) == len(tf_weights)\n",
    "        weighted = [embeddings[term] * w for term, w in zip(valid_keys, tf_weights)]\n",
    "        docvec = np.vstack(weighted)\n",
    "        docvec = np.sum(docvec, axis=0)\n",
    "        vecs[i,:] = docvec\n",
    "    return vecs\n",
    "\n",
    "#Here, we pass the dataframe column itself since each element inside our df is a list of tokens\n",
    "weighted_preTrained_Glove_dvs = weighted_docvecs(preTrained_Glove_wv, tfidf_weights, data[\"Processed Reviews\"])\n",
    "weighted_preTrained_Glove_dvs[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2cee2eee-280c-4f75-be7d-2e0e413ce811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19652, 50)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_preTrained_Glove_dvs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ab25a-9a28-48f2-b46f-6dd578724f05",
   "metadata": {},
   "source": [
    "Now that we have all the embeddings.\n",
    "- Bag of Words\n",
    "- Unweighted\n",
    "- Weighted\n",
    "\n",
    "Let's build Classfication models based on our Vector Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2442ce-3dc0-4ac7-a88a-acd87cfb44b1",
   "metadata": {},
   "source": [
    "Logisitic Regression based - Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "65ceb8f3-1973-431c-887d-a5f4c3702ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8768761129483592"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "seed = 15\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split( count_features ,data['Recommended IND'], list(range(0,len(data))),test_size=0.2, random_state=seed)\n",
    "max_iter = 1000\n",
    "model = LogisticRegression(max_iter = max_iter,random_state=seed) # increase the max_iter to 2000 for convergence\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512681bf-3f9c-476d-9ef6-683d0baf5d0f",
   "metadata": {},
   "source": [
    "Let's Examine the result further using a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c6bd6eeb-05f9-4f97-9a50-ed26ebce9aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 23.52222222222222, 'Predicted')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "categories = [1,0] # this gives sorted set of unique label names\n",
    "\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=categories, yticklabels=categories) # creates a heatmap from the confusion matrix\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0948e3d0-8e4f-4179-ad58-b478b561c172",
   "metadata": {},
   "source": [
    "Now for the unweighted embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "19857ab7-e405-43c5-a860-b472403e0cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8145510048333757"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split( preTGloVe_dvs ,data['Recommended IND'], list(range(0,len(data))),test_size=0.2, random_state=seed)\n",
    "max_iter = 1000\n",
    "model = LogisticRegression(max_iter = max_iter,random_state=seed) \n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef5fe2-4a6d-4f55-9083-b260cc6c8020",
   "metadata": {},
   "source": [
    "Now for the weighted embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7a400c6f-280b-4f1d-955d-a575aec6cb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8173492749936403"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split( weighted_preTrained_Glove_dvs ,data['Recommended IND'], list(range(0,len(data))),test_size=0.2, random_state=seed)\n",
    "max_iter = 1000\n",
    "model = LogisticRegression(max_iter = max_iter,random_state=seed)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745ebbf-f415-4fbb-9b5f-0e18845e7238",
   "metadata": {},
   "source": [
    "Finally, let's fine tune this process using Kfold Cross Validation across all the models\n",
    "Also - I defined a evaluate function just to help ease the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65542a0f-f01f-4b5c-8db2-0e39e58c6833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "num_folds = 5\n",
    "# Initialise a 5 fold validation\n",
    "kf = KFold(n_splits= num_folds, random_state=seed, shuffle = True) \n",
    "def evaluate(X_train,X_test,y_train, y_test,seed, max_iter):\n",
    "    model = LogisticRegression(max_iter =  max_iter,random_state=seed)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f226ed-9696-49dd-b478-12b3c61eac99",
   "metadata": {},
   "source": [
    "Let's test the validation and store the results into a seperate df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bae61630-f179-4622-93d7-cf2f3fb8cd29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Un-weighted</th>\n",
       "      <th>Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.876876</td>\n",
       "      <td>0.814297</td>\n",
       "      <td>0.817349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.869499</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.826507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.875827</td>\n",
       "      <td>0.824936</td>\n",
       "      <td>0.822901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.871247</td>\n",
       "      <td>0.83028</td>\n",
       "      <td>0.827226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875827</td>\n",
       "      <td>0.838168</td>\n",
       "      <td>0.831298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Count Un-weighted  Weighted\n",
       "0  0.876876    0.814297  0.817349\n",
       "1  0.869499    0.823963  0.826507\n",
       "2  0.875827    0.824936  0.822901\n",
       "3  0.871247     0.83028  0.827226\n",
       "4  0.875827    0.838168  0.831298"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_models = 3\n",
    "# Creates a dataframe to store the accuracy scores in all the folds\n",
    "cv_df = pd.DataFrame(columns = ['Count','Un-weighted','Weighted'],index=range(num_folds)) \n",
    "\n",
    "seed = 0\n",
    "max_iter =  2000\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(list(range(0,len(data)))):\n",
    "    y_train = [data['Recommended IND'].iloc[i] for i in train_index]\n",
    "    y_test = [data['Recommended IND'].iloc[i] for i in test_index]\n",
    "\n",
    "    X_train_binary, X_test_binary = count_features[train_index], count_features[test_index]\n",
    "    cv_df.loc[fold,'Count'] = evaluate(count_features[train_index],count_features[test_index],y_train,y_test,seed,max_iter =  max_iter)\n",
    "    \n",
    "    X_train_count, X_test_count = preTGloVe_dvs[train_index], preTGloVe_dvs[test_index]\n",
    "    cv_df.loc[fold,'Un-weighted'] = evaluate(preTGloVe_dvs[train_index],preTGloVe_dvs[test_index],y_train,y_test,seed, max_iter =  max_iter)\n",
    "\n",
    "    X_train_tfidf, X_test_tfidf = weighted_preTrained_Glove_dvs[train_index], weighted_preTrained_Glove_dvs[test_index]\n",
    "    cv_df.loc[fold,'Weighted'] = evaluate(weighted_preTrained_Glove_dvs[train_index],weighted_preTrained_Glove_dvs[test_index],y_train,y_test,seed, max_iter =  max_iter)\n",
    "\n",
    "    fold +=1\n",
    "\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ccfd503c-ad50-4f89-8021-95540c393828",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Count          0.873855\n",
       "Un-weighted    0.826329\n",
       "Weighted       0.825056\n",
       "dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc9f56-1e3b-4c67-8ee2-ae7ec4d06c2f",
   "metadata": {},
   "source": [
    "Alright, now let's do all the same for the \"Title column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "930fbec9-7ffb-4093-9708-b806b2a44dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  3907\n",
      "Total number of tokens:  65341\n",
      "Lexical diversity:  0.05979400376486433\n",
      "Total number of reviews: 19652\n",
      "Average review length: 3.3249033177284755\n",
      "Maximun review length: 12\n",
      "Minimun review length: 0\n",
      "Standard deviation of review length: 1.7797869477257426\n"
     ]
    }
   ],
   "source": [
    "exp = 20\n",
    "def tokenizetitle(review):\n",
    "    # This function tokenizes with the given regular expression.  \n",
    "    # Cover all words to lowercase\n",
    "    new_rev = review.lower() \n",
    "    pattern = r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = nltk.RegexpTokenizer(pattern) \n",
    "    tokenised_review = tokenizer.tokenize(new_rev)\n",
    "    return tokenised_review\n",
    "\n",
    "# Defining a statistics function to keep track of statistics\n",
    "def stats_print(reviews):\n",
    "    # We put all the tokens in the corpus in a single list\n",
    "    words = list(chain.from_iterable(reviews)) \n",
    "    # Compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    vocab = set(words) \n",
    "    lexical_diversity = len(vocab)/len(words)\n",
    "    print(\"Vocabulary size: \",len(vocab))\n",
    "    print(\"Total number of tokens: \", len(words))\n",
    "    print(\"Lexical diversity: \", lexical_diversity)\n",
    "    print(\"Total number of reviews:\", len(reviews))\n",
    "    lens = [len(review) for review in reviews]\n",
    "    print(\"Average review length:\", np.mean(lens))\n",
    "    print(\"Maximun review length:\", np.max(lens))\n",
    "    print(\"Minimun review length:\", np.min(lens))\n",
    "    print(\"Standard deviation of review length:\", np.std(lens))\n",
    "\n",
    "\n",
    "def removeLessFreqWords(review):\n",
    "    return [w for w in review if w not in lessFreqWords]\n",
    "\n",
    "def removeTopdocFreqWords(review):\n",
    "    return [w for w in review if w not in top20_docfreq_words]\n",
    "\n",
    "title_df = data[\"Title\"]\n",
    "title_data = [title for title in title_df]\n",
    "tokenized_titles = [tokenizetitle(title) for title in title_data]\n",
    "\n",
    "stats_print(tokenized_titles)\n",
    "\n",
    "# Iterate over tokenized_reviews and update each review\n",
    "for i, title in enumerate(tokenized_titles):\n",
    "    # Create a new list with words of length greater than 2\n",
    "    new_title = [word for word in title if len(word) >= 2] \n",
    "    # Update the tokenized_reviews list with the filtered words\n",
    "    tokenized_titles[i] = new_title\n",
    "\n",
    "# Removing Stopwords\n",
    "stopwords_titles = []\n",
    "with open('stopwords_en.txt') as f:\n",
    "    stopwords_titles = f.read().splitlines()\n",
    "\n",
    "# Converting the stopwords list into a set for better time complexity\n",
    "stopwordSet_titles = set(stopwords_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77401da-5dc5-4090-a4df-1573d8e9a5e1",
   "metadata": {},
   "source": [
    "Now, calculating the term-frequency & the document-frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6a49e332-39ad-49f7-ad08-d9e6858e8275",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  1608\n",
      "Total number of tokens:  24611\n",
      "Lexical diversity:  0.06533663808865954\n",
      "Total number of reviews: 19652\n",
      "Average review length: 1.252340728679015\n",
      "Maximun review length: 6\n",
      "Minimun review length: 0\n",
      "Standard deviation of review length: 1.0078978352641619\n"
     ]
    }
   ],
   "source": [
    "tokenized_titles = [[w for w in review if w not in stopwordSet_titles] \\\n",
    "                      for review in tokenized_titles]\n",
    "\n",
    "from nltk.probability import *\n",
    "title_words = list(chain.from_iterable(tokenized_titles))\n",
    "term_fd = FreqDist(title_words)\n",
    "\n",
    "lessFreqWords = set(term_fd.hapaxes())\n",
    "tokenized_titles = [removeLessFreqWords(title) for title in tokenized_titles]\n",
    "\n",
    "title_words_2 = list(chain.from_iterable([set(title) for title in tokenized_titles]))\n",
    "doc_fd = FreqDist(title_words_2) \n",
    "\n",
    "# Removing the top 20 words\n",
    "top20_docfreq = doc_fd.most_common(20)\n",
    "\n",
    "top20_docfreq_words = []\n",
    "# Appending only the word from the freq_list\n",
    "for word in top20_docfreq:\n",
    "    top20_docfreq_words.append(word[0])\n",
    "\n",
    "# Parsing the list to a set for better lookup time\n",
    "top20_docfreq_words = (top20_docfreq_words)\n",
    "\n",
    "tokenized_titles = [removeTopdocFreqWords(title) for title in tokenized_titles]\n",
    "\n",
    "stats_print(tokenized_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b8ea709-6b1d-429c-90c7-35b61fa8703c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JAY\\AppData\\Local\\Temp\\ipykernel_23240\\2291586404.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"Processed Titles\"] = tokenized_titles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>Processed Reviews</th>\n",
       "      <th>Processed Titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[high, hopes, wanted, work, initially, petite,...</td>\n",
       "      <td>[major, design, flaws]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it's fun, fl...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>[jumpsuit, fun, flirty, fabulous, time, compli...</td>\n",
       "      <td>[favorite, buy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>[shirt, due, adjustable, front, tie, length, l...</td>\n",
       "      <td>[shirt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1080</td>\n",
       "      <td>49</td>\n",
       "      <td>Not for the very petite</td>\n",
       "      <td>I love tracy reese dresses, but this one is no...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[tracy, reese, dresses, petite, feet, tall, br...</td>\n",
       "      <td>[petite]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858</td>\n",
       "      <td>39</td>\n",
       "      <td>Cagrcoal shimmer fun</td>\n",
       "      <td>I aded this in my basket at hte last mintue to...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[basket, hte, person, store, pick, teh, pale, ...</td>\n",
       "      <td>[shimmer, fun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19657</th>\n",
       "      <td>1104</td>\n",
       "      <td>34</td>\n",
       "      <td>Great dress for many occasions</td>\n",
       "      <td>I was very happy to snag this dress at such a ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[happy, snag, price, easy, slip, cut, combo]</td>\n",
       "      <td>[occasions]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19658</th>\n",
       "      <td>862</td>\n",
       "      <td>48</td>\n",
       "      <td>Wish it was made of cotton</td>\n",
       "      <td>It reminds me of maternity clothes. soft, stre...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Knits</td>\n",
       "      <td>[reminds, maternity, clothes, stretchy, shiny,...</td>\n",
       "      <td>[made, cotton]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19659</th>\n",
       "      <td>1104</td>\n",
       "      <td>31</td>\n",
       "      <td>Cute, but see through</td>\n",
       "      <td>This fit well, but the top was very see throug...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[worked, glad, store, order, online]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19660</th>\n",
       "      <td>1084</td>\n",
       "      <td>28</td>\n",
       "      <td>Very cute dress, perfect for summer parties an...</td>\n",
       "      <td>I bought this dress for a wedding i have this ...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[wedding, summer, medium, waist, perfectly, lo...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19661</th>\n",
       "      <td>1104</td>\n",
       "      <td>52</td>\n",
       "      <td>Please make more like this one!</td>\n",
       "      <td>This dress in a lovely platinum is feminine an...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>[lovely, feminine, perfectly, easy, comfy, hig...</td>\n",
       "      <td>[make]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19652 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Clothing ID  Age                                              Title  \\\n",
       "0             1077   60                            Some major design flaws   \n",
       "1             1049   50                                   My favorite buy!   \n",
       "2              847   47                                   Flattering shirt   \n",
       "3             1080   49                            Not for the very petite   \n",
       "4              858   39                               Cagrcoal shimmer fun   \n",
       "...            ...  ...                                                ...   \n",
       "19657         1104   34                     Great dress for many occasions   \n",
       "19658          862   48                         Wish it was made of cotton   \n",
       "19659         1104   31                              Cute, but see through   \n",
       "19660         1084   28  Very cute dress, perfect for summer parties an...   \n",
       "19661         1104   52                    Please make more like this one!   \n",
       "\n",
       "                                             Review Text  Rating  \\\n",
       "0      I had such high hopes for this dress and reall...       3   \n",
       "1      I love, love, love this jumpsuit. it's fun, fl...       5   \n",
       "2      This shirt is very flattering to all due to th...       5   \n",
       "3      I love tracy reese dresses, but this one is no...       2   \n",
       "4      I aded this in my basket at hte last mintue to...       5   \n",
       "...                                                  ...     ...   \n",
       "19657  I was very happy to snag this dress at such a ...       5   \n",
       "19658  It reminds me of maternity clothes. soft, stre...       3   \n",
       "19659  This fit well, but the top was very see throug...       3   \n",
       "19660  I bought this dress for a wedding i have this ...       3   \n",
       "19661  This dress in a lovely platinum is feminine an...       5   \n",
       "\n",
       "       Recommended IND  Positive Feedback Count   Division Name  \\\n",
       "0                    0                        0         General   \n",
       "1                    1                        0  General Petite   \n",
       "2                    1                        6         General   \n",
       "3                    0                        4         General   \n",
       "4                    1                        1  General Petite   \n",
       "...                ...                      ...             ...   \n",
       "19657                1                        0  General Petite   \n",
       "19658                1                        0  General Petite   \n",
       "19659                0                        1  General Petite   \n",
       "19660                1                        2         General   \n",
       "19661                1                       22  General Petite   \n",
       "\n",
       "      Department Name Class Name  \\\n",
       "0             Dresses    Dresses   \n",
       "1             Bottoms      Pants   \n",
       "2                Tops    Blouses   \n",
       "3             Dresses    Dresses   \n",
       "4                Tops      Knits   \n",
       "...               ...        ...   \n",
       "19657         Dresses    Dresses   \n",
       "19658            Tops      Knits   \n",
       "19659         Dresses    Dresses   \n",
       "19660         Dresses    Dresses   \n",
       "19661         Dresses    Dresses   \n",
       "\n",
       "                                       Processed Reviews  \\\n",
       "0      [high, hopes, wanted, work, initially, petite,...   \n",
       "1      [jumpsuit, fun, flirty, fabulous, time, compli...   \n",
       "2      [shirt, due, adjustable, front, tie, length, l...   \n",
       "3      [tracy, reese, dresses, petite, feet, tall, br...   \n",
       "4      [basket, hte, person, store, pick, teh, pale, ...   \n",
       "...                                                  ...   \n",
       "19657       [happy, snag, price, easy, slip, cut, combo]   \n",
       "19658  [reminds, maternity, clothes, stretchy, shiny,...   \n",
       "19659               [worked, glad, store, order, online]   \n",
       "19660  [wedding, summer, medium, waist, perfectly, lo...   \n",
       "19661  [lovely, feminine, perfectly, easy, comfy, hig...   \n",
       "\n",
       "             Processed Titles  \n",
       "0      [major, design, flaws]  \n",
       "1             [favorite, buy]  \n",
       "2                     [shirt]  \n",
       "3                    [petite]  \n",
       "4              [shimmer, fun]  \n",
       "...                       ...  \n",
       "19657             [occasions]  \n",
       "19658          [made, cotton]  \n",
       "19659                      []  \n",
       "19660                      []  \n",
       "19661                  [make]  \n",
       "\n",
       "[19652 rows x 12 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Processed Titles\"] = tokenized_titles\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac64276a-298c-4de8-a084-ab2ef961d855",
   "metadata": {},
   "source": [
    "Now, building the vector representation for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b9e52f2-ddaf-4c3a-a2f5-9ac41ede6f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Titles: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Un-weighted</th>\n",
       "      <th>Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.876876</td>\n",
       "      <td>0.814297</td>\n",
       "      <td>0.817349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.869499</td>\n",
       "      <td>0.823963</td>\n",
       "      <td>0.826507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.875827</td>\n",
       "      <td>0.824936</td>\n",
       "      <td>0.822901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.871247</td>\n",
       "      <td>0.83028</td>\n",
       "      <td>0.827226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875827</td>\n",
       "      <td>0.838168</td>\n",
       "      <td>0.831298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Count Un-weighted  Weighted\n",
       "0  0.876876    0.814297  0.817349\n",
       "1  0.869499    0.823963  0.826507\n",
       "2  0.875827    0.824936  0.822901\n",
       "3  0.871247     0.83028  0.827226\n",
       "4  0.875827    0.838168  0.831298"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles = [' '.join(review) for review in data[\"Processed Titles\"]]\n",
    "\n",
    "num_models = 3\n",
    "# Creates a dataframe to store the accuracy scores in all the folds\n",
    "cv_df = pd.DataFrame(columns = ['Count','Un-weighted','Weighted'],index=range(num_folds)) \n",
    "\n",
    "seed = 0\n",
    "max_iter =  2000\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(list(range(0,len(data)))):\n",
    "    y_train = [data['Recommended IND'].iloc[i] for i in train_index]\n",
    "    y_test = [data['Recommended IND'].iloc[i] for i in test_index]\n",
    "\n",
    "    X_train_binary, X_test_binary = count_features[train_index], count_features[test_index]\n",
    "    cv_df.loc[fold,'Count'] = evaluate(count_features[train_index],count_features[test_index],y_train,y_test,seed,max_iter =  max_iter)\n",
    "    \n",
    "    X_train_count, X_test_count = preTGloVe_dvs[train_index], preTGloVe_dvs[test_index]\n",
    "    cv_df.loc[fold,'Un-weighted'] = evaluate(preTGloVe_dvs[train_index],preTGloVe_dvs[test_index],y_train,y_test,seed, max_iter =  max_iter)\n",
    "\n",
    "    X_train_tfidf, X_test_tfidf = weighted_preTrained_Glove_dvs[train_index], weighted_preTrained_Glove_dvs[test_index]\n",
    "    cv_df.loc[fold,'Weighted'] = evaluate(weighted_preTrained_Glove_dvs[train_index],weighted_preTrained_Glove_dvs[test_index],y_train,y_test,seed, max_iter =  max_iter)\n",
    "\n",
    "    fold +=1\n",
    "\n",
    "print(\"For Titles: \")\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b00b57-1ed6-4687-ae3e-6193d8f94196",
   "metadata": {},
   "source": [
    "## Results\n",
    "Alright, we have successfully implemented 3 types of vector representations bag-of-words, Unweighted & weighted models. We used several techniques & methods such as term-frequency, document-frequency and generated weights for our data. We used that data to create embeddings using different methods as mentioned above. Finally we built a classification model with a decent accuracy. We repeated the same process for the \"Title column\" as well.  This implementation demonstrates a complete NLP pipeline from raw text processing to predictive modeling, showcasing various techniques for text representation in machine learning.\n",
    "\n",
    "Well, This classifer can now be used to classify reviews (Clothing reviews). This is how complex statements such as user reviews are classified to keep track of the sentiment towards the product. That being said, this isn't the best method pretty sure we can build more models on this. However, This is the gist of all the processing required for classiying something like reviews.\n",
    "\n",
    "Hope it helps. Thanks to RMIT University for providing the resources & the opportunity to work on such as assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
